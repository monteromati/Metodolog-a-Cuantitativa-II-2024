---
title: "Taller 3a"
format: html
editor: 
  markdown: 
    wrap: 72
---

![](logo-ie-ciae.gif){fig-align="left" width="200"}

# An谩lisis de Regresi贸n Lineal 

En esta sesi贸n, aprender谩s a realizar un an谩lisis de regresi贸n lineal simple y m煤ltiple utilizando datos de PISA 2009 para Chile. Exploraremos c贸mo evaluar los supuestos del modelo de regresi贸n, interpretar los resultados y visualizar las relaciones entre las variables. Tambi茅n, discutiremos c贸mo comparar diferentes modelos de regresi贸n y c贸mo interpretar la importancia relativa de los predictores.

Al igual que en las sesiones previas, es importante comenzar estableciendo el directorio de trabajo, la cual refiere a la carpeta de tu computador donde se encuentran los datos que vamos a analizar.

```{r}
getwd()
setwd("G:/Mi unidad/Teaching/Metodolog铆a Cuantitativa II 2024/Sesi贸n 3") # Cambia esta l铆nea de c贸digo por tu directorio de trabajo
```

## Instalar y Cargar Paquetes

Aprenderemos una forma alternativa de instalar y cargar paquetes usando `pacman`. El c贸digo a continuaci贸n verifica si el paquete `pacman` est谩 instalado y, si no lo est谩, lo instala. Luego, instala y carga todos los paquetes mencionados utilizando `pacman::p_load`, que instala y carga autom谩ticamente cualquier paquete que no est茅 ya instalado. Siempre puedes tambi茅n ocupar las funciones de R base de `install.packages()` y `library()`, aunque como ves esta es una alternativa m谩s f谩cil de leer, especialente cuando trabajas con muchos paquetes.

```{r}
if(!require(pacman)) install.packages("pacman")
pacman::p_load(haven,
               ggplot2,
               gmodels,
               psych,
               corrplot,
               lmtest,
               broom,
               relaimpo,
               visreg,
               performance,
               see,
               patchwork,
               knitr,
               car)
```

## Importar Datos

A continuaci贸n, importamos los datos de PISA 2009 para Chile. Utilizaremos el paquete `haven` para manejar los archivos SPSS (.sav).

```{r}
pisa2009_chl <- read_sav("pisa2009_chl.sav")
```

## Exploraci贸n Inicial de los Datos

Antes de proceder al an谩lisis, es importante entender la estructura y el contenido del conjunto de datos.

```{r}
class(pisa2009_chl)    # Verifica la clase del objeto 'pisa2009_chl'
head(pisa2009_chl)     # Muestra los primeros 6 casos de la base de datos
names(pisa2009_chl)    # Lista los nombres de las variables
dim(pisa2009_chl)      # Muestra el n煤mero de variables y casos
summary(pisa2009_chl)  # Proporciona un resumen estad铆stico para cada variable
str(pisa2009_chl)      # Muestra la estructura interna del data frame
```

## An谩lisis Descriptivo

### Estad铆sticos Descriptivos

Comenzamos con el an谩lisis descriptivo del ndice de Estatus Econ贸mico, Social y Cultural (ESCS, por sus siglas en ingl茅s en PISA) o simplemente estatus socioecon贸mico del estudiantes (`ses`) y del puntaje en matem谩ticas (`math`).

```{r}
pisa2009_chl$ses <- as.numeric(pisa2009_chl$ses)
summary(pisa2009_chl$ses)  # Resumen descriptivo de SES
sd(pisa2009_chl$ses, na.rm = TRUE)  # Desviaci贸n est谩ndar de SES
var(pisa2009_chl$ses, na.rm = TRUE)  # Varianza de SES
describe(pisa2009_chl$ses)

summary(pisa2009_chl$math)  # Resumen descriptivo de math
sd(pisa2009_chl$math, na.rm = TRUE)  # Desviaci贸n est谩ndar de math
var(pisa2009_chl$math, na.rm = TRUE)  # Varianza de math
describe(pisa2009_chl$math)
```

### Visualizaci贸n de la Distribuci贸n de los Datos

Para entender mejor la distribuci贸n de los datos, generamos varios gr谩ficos usando el paquete `ggplot2`.

```{r}
# Histograma de SES
ggplot(pisa2009_chl, aes(x = ses)) +
  geom_histogram()

# Gr谩fico de Densidad de SES
ggplot(pisa2009_chl, aes(x = ses)) +
  geom_density()

# Histograma y Densidad combinados
ggplot(pisa2009_chl, aes(x = ses)) +
  geom_histogram(aes(y = ..density..)) +
  geom_density()

# Histograma de Math
ggplot(pisa2009_chl, aes(x = math)) +
  geom_histogram()

# Gr谩fico de Densidad de Math
ggplot(pisa2009_chl, aes(x = math)) +
  geom_density()

# Histograma y Densidad combinados
ggplot(pisa2009_chl, aes(x = math)) +
  geom_histogram(aes(y = ..density..)) +
  geom_density()
```

## An谩lisis de Regresi贸n Lineal

### Regresi贸n Lineal Simple

Realizaremos una regresi贸n lineal simple para predecir el puntaje en matem谩ticas (math) utilizando el 铆ndice de estatus socioecon贸mico como predictor.

```{r}
m1 <- lm(math ~ ses, data = pisa2009_chl)
summary(m1)
```

#### Evaluaci贸n de los Supuestos del Modelo de Regresi贸n

Es fundamental evaluar los supuestos estad铆sticos del modelo de regresi贸n para asegurarnos de que los resultados sean v谩lidos.

-   **Supuesto de Linealidad**
El supuesto de linealidad indica que la relaci贸n entre el predictor (ses) y la variable dependiente (math) es lineal. Para evaluar la linealidad, podemos utilizar un diagrama de dispersi贸n (scatter plot) que muestre la relaci贸n entre la variable predictora `ses` (ndice de Estatus Econ贸mico, Social y Cultural) y la variable dependiente `math` (Puntaje en Matem谩ticas).

```{r}
ggplot(pisa2009_chl) +
  aes(x = ses, y = math) +
  geom_point(colour = "#0c4c8a") +
  theme_minimal()
```

*Interpretaci贸n:* Si los puntos en el gr谩fico muestran una tendencia a alinearse en una l铆nea recta (ascendente o descendente), podemos asumir que la relaci贸n es aproximadamente lineal. Si los puntos siguen una forma curvil铆nea, la relaci贸n podr铆a no ser lineal, lo que sugerir铆a la necesidad de transformar las variables o considerar un modelo diferente.

![](correlation_examples2.png){fig-align="left" width="400"}

-   **Supuesto de Normalidad de los Residuos**
El supuesto de normalidad de los residuos indica que los errores del modelo (es decir, las diferencias entre los valores observados y los valores predichos) deben distribuirse normalmente. Esto es crucial para validar las pruebas de significancia estad铆stica (prueba **t** de Student) que se utiliza en la regresi贸n. Para evaluar este supuesto, utilizamos varias aproximaciones:

    -   Histograma de los residuos: Permite observar la distribuci贸n de los errores.
    -   Gr谩fico de densidad: Proporciona una visualizaci贸n suave de la distribuci贸n de los residuos.
    -   Gr谩fico Cuantil-Cuantil (Q-Q plot): Compara la distribuci贸n de los residuos con una distribuci贸n normal te贸rica.
    -   Prueba de Shapiro-Wilk: Prueba estad铆stica que eval煤a la normalidad de los residuos.

```{r}
hist(m1$residuals, freq = FALSE)
plot(density(m1$residuals))
qqnorm(m1$residuals, main = 'Gr谩fico Cuantil-Cuantil')
qqline(m1$residuals)
shapiro.test(m1$residuals[0:5000])
```

-   **Supuesto de Homocedasticidad**
El supuesto de homocedasticidad establece que la varianza de los residuos es constante a lo largo de todos los valores de la variable predictora. Esto significa que los errores del modelo no deber铆an mostrar patrones sistem谩ticos cuando se grafican contra los valores predichos. La falta de homocedasticidad, conocida como heterocedasticidad, puede llevar a estimaciones sesgadas de los coeficientes de regresi贸n.

Para evaluar este supuesto, utilizamos el test de Breusch-Pagan, que es una prueba estad铆stica que eval煤a la presencia de heterocedasticidad. Si el test de Breusch-Pagan indica Un valor *p* > .05, entonces este sugiere que no se puede rechazar la hip贸tesis nula de homocedasticidad, indicando que la varianza de los residuos es constante. Si el valor *p* es menor a 0.05, hay evidencia de heterocedasticidad, lo que podr铆a requerir la transformaci贸n de variables o el uso de un modelo robusto.

```{r}
bptest(math ~ ses, data = pisa2009_chl)
```

### Regresi贸n Lineal M煤ltiple
A continuaci贸n, realizamos una regresi贸n lineal m煤ltiple, agregando m谩s predictores al modelo.

```{r}
# Recodificaci贸n de la variable sexo
pisa2009_chl$female <- as.numeric(pisa2009_chl$sex)
pisa2009_chl$female[pisa2009_chl$female == 0] <- 0
pisa2009_chl$female[pisa2009_chl$female == 1] <- 1

m2 <- lm(math ~ ses + female + read + scie, data = pisa2009_chl)
summary(m2)
```

-   **Evaluaci贸n de la Multicolinealidad**
En este caso como tenemos 2 o m谩s predictores, debemos evaluar la multicolinealidad en el modelo. La multicolinealidad ocurre cuando dos o m谩s variables predictoras en un modelo de regresi贸n est谩n altamente correlacionadas entre s铆. Esto puede ser problem谩tico porque impide la estimaci贸n de un modelo de regresi贸n. Para ello, observamos las correlaciones entre los predictores y calculamos el factor de inflaci贸n de la varianza (VIF).

Podemos evaluar la presencia de multicolinealidad en un modelo de regresi贸n generando una matriz de correlaci贸n de los predictores puede dar una idea inicial sobre las relaciones bivariadas entre ellos. Observamos si hay correlaciones altas (generalmente **r** > 0.8 o < -0.8) entre dos variables predictoras pueden ser una indicador de multicolinealidad.

```{r}
data <- pisa2009_chl[, c("math", "ses", "read", "scie")]
correlaciones <- cor(data, use = "complete.obs")

# Visualizar la matriz de correlaci贸n usando un correlograma
corrplot(correlaciones, method = 'circle', type = 'lower', insig = 'blank',
         addCoef.col = 'black', number.cex = 0.8, order = 'AOE', diag = FALSE)
```

Tambi茅n, podemos estimar el factor de inflaci贸n de la varianza (VIF). El VIF mide cu谩nto la varianza de un coeficiente de regresi贸n est谩 inflada debido a la multicolinealidad. Un VIF de 1 indica que no hay correlaci贸n entre la variable predictora y las dem谩s variables predictoras. Un VIF entre 1 y 5 sugiere una correlaci贸n moderada pero aceptable. Un VIF mayor que 5 (algunos usan el umbral de 10) indica una alta multicolinealidad que podr铆a ser problem谩tica.

```{r}
kable(vif(m2), digits = 2)
```

Interpretaci贸n: Si alguna de las variables en el modelo tiene un VIF alto (por ejemplo, mayor a 5), esto indica que la variable est谩 altamente correlacionada con una o m谩s de las otras variables predictoras, lo que sugiere multicolinealidad.

**驴Qu茅 hacer si hay multicolinealidad?**
-   Eliminar una de las variables correlacionadas: Si dos variables est谩n altamente correlacionadas, podr铆as considerar eliminar una de ellas para reducir la multicolinealidad.
-   Transformar las variables: En algunos casos, la transformaci贸n de las variables (por ejemplo, usando logaritmos o diferencias) puede reducir la multicolinealidad.

Esta funci贸n obtiene gr谩ficos de diagn贸stico para chequear supuestos.

```{r}
check_model(m1)
```

**Comparaci贸n de Modelos**
Finalmente, comparamos los dos modelos de regresi贸n para determinar cu谩l ajusta mejor los datos.

```{r}
anova(m1, m2)
```

**Importancia Relativa de los Predictores**
Calculamos la importancia relativa de cada predictor en el modelo de regresi贸n m煤ltiple.

```{r}
calc.relimp(m2, type = c("lmg", "last", "first", "pratt"), rela = TRUE)
```